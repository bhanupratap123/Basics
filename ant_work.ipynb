{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ant_work.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanupratap123/Basics/blob/master/ant_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jULYK2jow-Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnAqdrpwdHpi",
        "colab_type": "code",
        "outputId": "978abc31-5bc2-489a-8805-bc9cb1371222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#libraries Import needed for the project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwuigg3jdHp7",
        "colab_type": "code",
        "outputId": "bbfd91a3-36bb-4c1d-9c19-98525a3d85cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "source": [
        "#import the data set and columns needed for the project\n",
        "df_Train = pd.read_csv(\"train.csv\",sep = \"~\",encoding = 'utf-8',usecols=['Description','Is_Response'])\n",
        "df_Test = pd.read_csv(\"test.csv\",sep = \"~\",encoding = 'utf-8',usecols=['User_ID','Description'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f2c7f1835eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_Train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Is_Response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'User_ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1mB-5jydHqL",
        "colab_type": "code",
        "outputId": "f11df378-df6c-4f5e-a48a-98332069159c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "#print the useful information about the dataset\n",
        "print(\"train data\")\n",
        "print(df_Train.info())\n",
        "print(df_Train.head(10))\n",
        "print(\"test data\")\n",
        "print(df_Test.info())\n",
        "print(df_Test.head(3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cf25e0610dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_Train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5WQ7Xg3dHqe",
        "colab_type": "code",
        "outputId": "87793ab4-4bbf-466f-abb5-3bf25973e34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#check the class distribution\n",
        "Response_Train = df_Train[\"Is_Response\"]\n",
        "print(Response_Train.value_counts())\n",
        "#from this we can evaluate that number of Good count is more than number of Bad count\n",
        "#we dont have equal class distribution\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Good    20567\n",
            "Bad      9605\n",
            "Name: Is_Response, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrPILrQ5dHqp",
        "colab_type": "code",
        "outputId": "5507f732-f720-4575-ebc6-fbe47d317446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Convert the Good And Bad into Binary form, Good = 1 and Bad = 0\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "new_df = encoder.fit_transform(Response_Train)\n",
        "print(new_df[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 0 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQOrVrYAdHqy",
        "colab_type": "code",
        "outputId": "7cdbe688-fcdf-4620-83c0-26d6299ffcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Now preprocessing of the text message\n",
        "Description_Train = df_Train[\"Description\"]\n",
        "print(Description_Train[0:5])\n",
        "\n",
        "\n",
        "Description_Test = df_Test[\"Description\"]\n",
        "print(Description_Test[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    After reading mixed reviews I almost didn't bo...\n",
            "1    This motor inn is located about - city blocks ...\n",
            "2    It was our first time there and surely not our...\n",
            "3    Great hotel in an excellent location, just off...\n",
            "4    We stayed at the hotel for - weeks to get away...\n",
            "Name: Description, dtype: object\n",
            "0    A friend and I stayed in this hotel when we we...\n",
            "1    I enjoy staying here when I have early flights...\n",
            "2    I stopped off in Seattle during a train tour o...\n",
            "3    I have stayed at this hotel - or - times now f...\n",
            "4    Excellent location with hop on hop off city tr...\n",
            "Name: Description, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIjbkhsdHrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess(Description):\n",
        "  #Removing emails,phone number ,number ,symbols by regular expression \n",
        "\n",
        "  #removing the email adress witn \"email\"\n",
        "  processed_data = Description.str.replace(\"^\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,3}$\",'email')\n",
        "\n",
        "  #replace webadress ,URL with \"web\"\n",
        "  processed_data = processed_data.str.replace(\"^[a-zA-Z0-9\\-\\.]+\\.(com|org|net|mil|edu|COM|ORG|NET|MIL|EDU)$\",\"web\")\n",
        "\n",
        "  #replace phone number with a \"phone Number\"\n",
        "  processed_data = processed_data.str.replace(\"^[2-9]\\d{2}-\\d{3}-\\d{4}$\",\"phone number\")\n",
        "\n",
        "  #replace normal number with \"number\"\n",
        "  processed_data = processed_data.str.replace(\"^\\d{3}-\\d{2}-\\d{4}$\",\"number\")\n",
        "\n",
        "  # Remove punctuation\n",
        "  processed_data = processed_data.str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "\n",
        "  # Replace whitespace between terms with a single space\n",
        "  processed_data = processed_data.str.replace(r'\\s+', ' ')\n",
        "\n",
        "  # Remove leading and trailing whitespace\n",
        "  processed_data = processed_data.str.replace(r'^\\s+|\\s+?$', '')\n",
        "\n",
        "  #converting the words to the lower case\n",
        "  processed_data = processed_data.str.lower()\n",
        "  #removing the stopwords from the data\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  #removing the stopword\n",
        "  processed_data = processed_data.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
        "  \n",
        "  # Remove word stems using a Porter stemmer\n",
        "  ps = nltk.PorterStemmer()\n",
        "  \n",
        "  processed_data = processed_data.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
        "  #print(processed_data)\n",
        "  return processed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_3wXXudHrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_train_data = preprocess(Description_Train)\n",
        "processed_test_data = preprocess(Description_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3cMjDa-2W4Q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5rzskF6dHrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfK0K8ptdHrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrKN3B-ZdHr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6153JAEdHsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfo2Ae1RdHsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#after data preprocessing,Time for now is word tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# create bag-of-words\n",
        "all_words = []\n",
        "\n",
        "for message in processed_train_data:\n",
        "    words = word_tokenize(message)\n",
        "    for w in words:\n",
        "        all_words.append(w)\n",
        "        \n",
        "all_words = nltk.FreqDist(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjRLWlV5dHsd",
        "colab_type": "code",
        "outputId": "8a394926-282b-43d8-d59a-5072fae927e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# print the total number of words and the 15 most common words\n",
        "print('Number of words: {}'.format(len(all_words)))\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 28369\n",
            "Most common words: [('room', 64084), ('hotel', 61667), ('stay', 37548), ('great', 19954), ('staff', 19084), ('locat', 17841), ('night', 17209), ('would', 17147), ('nice', 15297), ('one', 15163), ('good', 14984), ('time', 14568), ('clean', 14109), ('bed', 13439), ('us', 12450)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGwIVtT3dHsp",
        "colab_type": "code",
        "outputId": "fd923247-9386-4b9a-d2d2-4226098af1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3768
        }
      },
      "source": [
        "# use the 1000 most common words as features\n",
        "word_features = list(all_words.keys())[:1000]\n",
        "\n",
        "# The find_features function will determine which of the 1000 word features are contained in the review\n",
        "def find_features(message):\n",
        "    words = word_tokenize(message)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Lets see an example!\n",
        "features = find_features(processed_train_data[0])\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print(key)\n",
        "print(\"Test data\")\n",
        "features = find_features(processed_test_data[0])\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print(key)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read\n",
            "mix\n",
            "review\n",
            "almost\n",
            "book\n",
            "w\n",
            "attend\n",
            "concert\n",
            "pantag\n",
            "theatr\n",
            "get\n",
            "conveni\n",
            "use\n",
            "mass\n",
            "transit\n",
            "liter\n",
            "top\n",
            "hollywood\n",
            "vine\n",
            "station\n",
            "take\n",
            "red\n",
            "line\n",
            "union\n",
            "attitud\n",
            "ye\n",
            "new\n",
            "hip\n",
            "hotel\n",
            "everi\n",
            "staff\n",
            "member\n",
            "met\n",
            "friendli\n",
            "total\n",
            "commit\n",
            "servic\n",
            "valet\n",
            "park\n",
            "front\n",
            "desk\n",
            "room\n",
            "men\n",
            "black\n",
            "secur\n",
            "folk\n",
            "need\n",
            "club\n",
            "scene\n",
            "everyon\n",
            "great\n",
            "food\n",
            "alway\n",
            "sampl\n",
            "first\n",
            "time\n",
            "properti\n",
            "arriv\n",
            "earlier\n",
            "promis\n",
            "impress\n",
            "onion\n",
            "soup\n",
            "gratine\n",
            "simpli\n",
            "best\n",
            "ever\n",
            "simpl\n",
            "prime\n",
            "beef\n",
            "cheeseburg\n",
            "hand\n",
            "form\n",
            "patti\n",
            "light\n",
            "bun\n",
            "usual\n",
            "trim\n",
            "condiment\n",
            "french\n",
            "fri\n",
            "never\n",
            "trendi\n",
            "well\n",
            "appoint\n",
            "comfort\n",
            "univers\n",
            "cell\n",
            "phone\n",
            "gizmo\n",
            "charger\n",
            "handi\n",
            "worri\n",
            "parti\n",
            "nois\n",
            "spend\n",
            "extra\n",
            "buck\n",
            "exterior\n",
            "face\n",
            "troubl\n",
            "summari\n",
            "would\n",
            "come\n",
            "absolut\n",
            "Test data\n",
            "concert\n",
            "get\n",
            "use\n",
            "top\n",
            "line\n",
            "new\n",
            "hotel\n",
            "total\n",
            "front\n",
            "desk\n",
            "room\n",
            "food\n",
            "alway\n",
            "time\n",
            "comfort\n",
            "worri\n",
            "would\n",
            "town\n",
            "small\n",
            "bed\n",
            "good\n",
            "bit\n",
            "flat\n",
            "free\n",
            "went\n",
            "drop\n",
            "pan\n",
            "enough\n",
            "rent\n",
            "floor\n",
            "could\n",
            "close\n",
            "night\n",
            "hard\n",
            "high\n",
            "us\n",
            "stay\n",
            "back\n",
            "eat\n",
            "thing\n",
            "littl\n",
            "gave\n",
            "coupon\n",
            "offer\n",
            "two\n",
            "right\n",
            "one\n",
            "nice\n",
            "definit\n",
            "made\n",
            "got\n",
            "peopl\n",
            "kick\n",
            "also\n",
            "heart\n",
            "sound\n",
            "internet\n",
            "charg\n",
            "guest\n",
            "paid\n",
            "cours\n",
            "go\n",
            "plenti\n",
            "spent\n",
            "around\n",
            "friend\n",
            "includ\n",
            "sink\n",
            "area\n",
            "utensil\n",
            "set\n",
            "late\n",
            "okay\n",
            "want\n",
            "money\n",
            "make\n",
            "though\n",
            "prefer\n",
            "can\n",
            "turn\n",
            "pm\n",
            "believ\n",
            "way\n",
            "word\n",
            "feel\n",
            "coupl\n",
            "warm\n",
            "suit\n",
            "item\n",
            "refriger\n",
            "easi\n",
            "look\n",
            "rememb\n",
            "given\n",
            "realli\n",
            "lot\n",
            "avail\n",
            "cool\n",
            "air\n",
            "quickli\n",
            "bother\n",
            "three\n",
            "smoke\n",
            "loud\n",
            "non\n",
            "elev\n",
            "wall\n",
            "thin\n",
            "sign\n",
            "seem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeGVwgLgdHsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets do it for all the messages\n",
        "messages_train = zip(processed_train_data,new_df)\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed = seed\n",
        "#np.random.shuffle(messages)\n",
        "\n",
        "# call find_features function for each SMS message\n",
        "featuresets_train = [(find_features(text), label) for (text, label) in messages_train]\n",
        "featuresets_test = [find_features(text) for text in processed_test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPDlCCYYdHtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can split the featuresets into training and testing datasets using sklearn\n",
        "from sklearn import model_selection\n",
        "\n",
        "# split the data into training and testing datasets\n",
        "training, validation = model_selection.train_test_split(featuresets_train, test_size = 0.25, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bhgj262dHtT",
        "colab_type": "code",
        "outputId": "16702d66-7041-4184-f6da-a0120cf60c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(training))\n",
        "print(len(validation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22629\n",
            "7543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZzEjAIKdHth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use sklearn algorithms in NLTK\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = SklearnClassifier(SVC(kernel = 'linear',verbose=3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCrtpdaidHtv",
        "colab_type": "code",
        "outputId": "fc2a1b0d-24cf-481d-f5b2-b0724a5b9fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\n",
        "# train the model on the training data\n",
        "model.train(training)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
              "  shrinking=True, tol=0.001, verbose=3))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rmrtWRedHt3",
        "colab_type": "code",
        "outputId": "a1ed836c-2887-45ed-8350-b23b4e79fa04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# and test on the testing dataset!\n",
        "accuracy = nltk.classify.accuracy(model, validation)*100\n",
        "print(\"Accuracy on validation data: {}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree Accuracy on validation data: 86.159353042556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMEXjmuIvYyM",
        "colab_type": "code",
        "outputId": "5291c57f-c7d2-4fdb-d3c3-858b6db519f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Model is trained and Validated. Now time to process the test data\n",
        "predictions = [model.classify(test_data_point) for test_data_point in featuresets_test]\n",
        "print(predictions[:10])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "staqJNQ1uEX7",
        "colab_type": "code",
        "outputId": "b10f1ae5-7c5d-4675-ac78-360bc6525696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction_labels = encoder.inverse_transform(predictions)\n",
        "print(prediction_labels[:10])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bad' 'Good' 'Good' 'Good' 'Good' 'Good' 'Bad' 'Good' 'Good' 'Good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZpG2nIyuef_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = zip(df_Test['User_ID'],prediction_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ochZ8-PxBP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_output = pd.DataFrame(list(output), columns = ['User_ID', 'Is_Response'])\n",
        "df_output.to_csv(\"submission.csv\", sep='~',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3QwVCFnxKdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}